# **Project Proposal (⚠ Unfinished)**  

## **Project Overview;**

The Culinary Companion is an innovative food-preparation assistant designed to revolutionize home cooking by utilizing advanced AI and motion tracking technology. The Culinary Companion learns from observing users as they prepare various dishes, enabling it to replicate cooking techniques, ingredient usage, and timing with precision. This project aims to create a robot that not only automates cooking tasks but also gives the user the freedom to add any of their own dishes to be prepared, and not be limited to a certain number of preset dishes.

## **Objectives;**

 - **Innovative Cooking Assistance:** Develop a robotic assistant that utilizes AI and motion tracking to learn and replicate user cooking techniques, enhancing the home cooking experience.

 - **User Customization:** Allow users to input their own recipes and cooking preferences, ensuring the system is not limited to preset dishes.

 - **Learning Capability:** Implement machine learning algorithms that enable the Culinary Companion to adapt and improve its cooking methods based on user feedback and observed practices, as well as personal judgment.

 - **User-Friendly Interface:** Design an intuitive interface that simplifies interaction with the Culinary Companion, making it easily accessible for users.

 - **Efficiency and Precision:** To enhance the cooking process's efficiency by optimizing ingredient use and cooking conditions, such as flame heat and timing.

## **Scope of Work;**

**Phase 1: Research:**

Since the setup will make use of hardware components such as mechanical arms to hold utensils and video cameras for footage, intensive research would be carried out regarding suitable hardware which is cost effective, while also maintains adequate quality of performance. Additionally, research into the camera’s limitations of colour identification and clarity would be required to decide what colour schemes each of the utensil and ingredient storage containers should hold for easy recognition.

**Phase 2: Back end system development**

- Observe and track user actions: Build a system that makes use of video input from cameras to record the user’s hand movement, track the position and movement of various utensils and containers of ingredients as well as observe the physical change in appearance of certain contents (if present).
- Analyse the recorded actions: Give the system the capability to differentiate between objects on the workspace by making use of distinct physical features, derive the angle each utensil was used at by the user and identify colour and temperature changes. Also recognise hand movement of the user to identify common actions (e.g. stirring, mashing, pouring etc.).
- Form a set of instructions: Allow the Algorithm to associate each action with certain objects used by the user, turn each of these elaborate actions into instructions which can be understood by the physical hardware. Then arrange these instructions in the order they were carried out by the user.
- Refine the instructions: The algorithm should then review each of the generated instructions in the set, and eliminate any unnecessary actions or make the existing actions smoother and more refined to ensure more accuracy, precision and time-efficiency.
- Communicate with mechanical hardware: Form a robust connection between the system’s software and hardware components to work effectively and perform instructions precisely. This involves the real-time monitoring of changes in appearance (if expected), timely mechanical arm control and accurate measurement of ingredients using apparatus on the work place.

**Phase 3: Front end system development**

## **Project Timeline;**

## **Conclusion;**
